<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_umyph4ytn2e9-1{list-style-type:none}ul.lst-kix_umyph4ytn2e9-2{list-style-type:none}ul.lst-kix_umyph4ytn2e9-0{list-style-type:none}.lst-kix_umyph4ytn2e9-8>li:before{content:"\0025a0  "}.lst-kix_umyph4ytn2e9-7>li:before{content:"\0025cb  "}.lst-kix_umyph4ytn2e9-1>li:before{content:"\0025cb  "}.lst-kix_umyph4ytn2e9-0>li:before{content:"\0025a0  "}.lst-kix_umyph4ytn2e9-2>li:before{content:"\0025a0  "}.lst-kix_umyph4ytn2e9-3>li:before{content:"\0025cf  "}ul.lst-kix_umyph4ytn2e9-7{list-style-type:none}.lst-kix_umyph4ytn2e9-4>li:before{content:"\0025cb  "}.lst-kix_umyph4ytn2e9-6>li:before{content:"\0025cf  "}ul.lst-kix_umyph4ytn2e9-8{list-style-type:none}ul.lst-kix_umyph4ytn2e9-5{list-style-type:none}ul.lst-kix_umyph4ytn2e9-6{list-style-type:none}ul.lst-kix_umyph4ytn2e9-3{list-style-type:none}ul.lst-kix_umyph4ytn2e9-4{list-style-type:none}.lst-kix_umyph4ytn2e9-5>li:before{content:"\0025a0  "}ol{margin:0;padding:0}table td,table th{padding:0}.c2{padding-top:-5pt;padding-bottom:12pt;line-height:1.3;orphans:2;widows:2}.c11{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c5{page-break-after:avoid;height:20pt}.c7{orphans:2;widows:2}.c8{color:#1155cc;text-decoration:underline}.c4{page-break-after:avoid;text-align:center}.c0{color:inherit;text-decoration:inherit}.c10{height:11pt}.c6{page-break-after:avoid}.c3{font-weight:700}.c9{text-indent:36pt}.c1{font-family:"Georgia"}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c11"><p class="c2 c4 title" id="h.ib1fd7q34vnu"><span>PixelDream</span></p><p class="c7 c4 subtitle" id="h.sohaj7fkc0cm"><span>James Reed and Kevin Malhotra</span></p><p class="c7 c4 subtitle" id="h.ykdup3ugyj7c"><span>ECE 4554/5554 Fall 2016</span></p><h1 class="c2 c5" id="h.e9js4aw02z33"><span class="c3"></span></h1><h1 class="c2 c6" id="h.e1p7s1rjnngg"><span class="c3">Abstract</span></h1><p class="c2"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The goal of this project was to gain understanding of PixelRNN&rsquo;s image modeling capabilities. We train the PixelRNN on the CIFAR10 dataset and subsequently ask the network to optimize a given new image to fit into the learned distribution. </span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 212.00px;"><img alt="" src="images/image10.png" style="width: 624.00px; height: 212.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c2 c6" id="h.rn0bahtvn33c"><span class="c3">Introduction</span></h1><p class="c2 c9"><span class="c1">Deep Learning has&mdash;in recent years&mdash;propelled the field of Computer Vision at a breakneck pace. However, a major disadvantage of these Neural Network models is that their inner workings are by and large opaque to Human inspection. Work by Google has provided insight into the workings of Convolutional Neural Network classifiers by using backpropagation to optimize the input signal&mdash;-rather than the weights&mdash;-to maximize the activation of a selected layer or neuron. The resulting input signals provide some visual intuition about what the network is doing. Recent work by DeepMind proposes the use of Recurrent Neural Networks for modeling the distribution of natural images. This is done by modeling the image as a discrete signal </span><img src="images/image00.png"><span class="c1">, where is a pixel value and is the dimensionality of the image. The model estimates the conditional probability for a discrete pixel value </span><img src="images/image01.png"><span class="c1">using an RNN model. We propose the application of the DeepDream methodology to the PixelRNN network architecture. Given an input image, we wish to adjust the image&rsquo;s pixel values such that it falls within the distribution learned by the PixelRNN network. </span></p><h1 class="c2 c6" id="h.3m516xawtmd2"><span class="c3">Approach</span><span>&nbsp;</span></h1><p class="c2 c9"><span class="c1">Generative image models take advantage of the fact that&mdash;although an image may be a very high-dimensional signal&mdash;the actual content of (natural) images resides on a lower-dimensional manifold. By training a neural network as a generative model, we obtain a model of the distribution of natural images. Given this model, we can perform various tasks such as sampling and imputation of missing data. Our goal is that given a trained generative model of images, we wish to present the model with an arbitrary image and have the model optimize the image such that it fits within the learned distribution.</span></p><p class="c2"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;To attain this goal, we used the recently-described PixelRNN architecture. This architecture represents the image as a sequence of pixels </span><img src="images/image02.png"><span class="c1">and thus models the conditional probability for each pixel value </span><img src="images/image03.png"><span class="c1">. Given a corpus of natural images&mdash;in our case the CIFAR10 dataset&mdash;we trained an existing Tensorflow implementation of the PixelRNN architecture on this dataset. The network is trained as an autoencoder such that the optimization objective is to match the input grayscale pixel values with the output probability map. Once the network is trained, we can iteratively sample from the learned distribution by sampling from each conditional pixel value probability until we&rsquo;ve constructed a full image.</span></p><p class="c2 c9"><span class="c1">Because Tensorflow is a flexible graph-based computation environment, we can easily reconfigure the network and apply pre-trained portions of the model to new tasks. In our case, we turned the optimization problem around. Given a pre-trained model, we have the network find </span><img src="images/image04.png"><span class="c1">via gradient descent. What is unique about our approach is that we provide the network with an initial input image </span><img src="images/image05.png"><span class="c1">&nbsp;from which we apply gradient descent. That is:</span></p><p class="c2"><img src="images/image06.png"></p><p class="c2"><img src="images/image07.png"></p><p class="c2"><span class="c1">}</span></p><p class="c2"><span class="c1">where </span><img src="images/image08.png"><span class="c1">&nbsp;is the </span><img src="images/image09.png"><span class="c1">loss between the input to the PixelRNN and the output probability map. In this way, we apply pixel value updates to the image until it fits within the learned distribution.</span></p><p class="c2 c10"><span class="c1"></span></p><h1 class="c2 c6" id="h.et4fj893iiv4"><span class="c3">Experiments and Results</span></h1><p class="c2"><span class="c1"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Images were input from CIFAR-10 test set into the PixelRNN to generate the modified input image to represent the model&rsquo;s distribution. The evaluation metrics were purely looking at the residuals between the input and the modified input image and viewing the modified regions that changed. The images were optimized until convergence of the loss function.</span></p><p class="c2"><span class="c1 c8"><a class="c0" href="https://www.google.com/url?q=https://github.com/jamesr66a/ECE5554Project&amp;sa=D&amp;ust=1481834748446000&amp;usg=AFQjCNFDJyTnotSpDhBGLZjECNgu3T5Q8w">Code Repository</a></span></p><h1 class="c2 c6" id="h.rasdxt1zpo4s"><span class="c3">Qualitative Results</span></h1><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 212.00px;"><img alt="" src="images/image10.png" style="width: 624.00px; height: 212.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c10"><span class="c1"></span></p><p class="c2"><span class="c1">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Qualitative results show a marked change in image regions that might be considered &ldquo;background&rdquo;. Intuitively, the CIFAR-10 dataset consists of images that represent a single subject and an irrelevant background. It appears that the model has learned this as part of the distribution of this training set, and the adjustments required to transform an input image to a conforming output image entails emphasizing the foreground subject while eliding background regions. </span></p><h1 class="c2 c6" id="h.5akumuevi1st"><span class="c3">Conclusion and Future Work</span><span>&nbsp;</span></h1><p class="c2"><span class="c1">The results have shown a priority placed on foreground images over the background images. The input dataset on CIFAR-10 has this distribution, therefore the PixelRNN has learned to model optimizing the foreground image over the background. One of the issues with our model was that we used Cross Entropy which led to optimizing the output space matching the input space with some entropy. However, this entropy not necessarily warranted and switching to optimizing the KL divergence will be sufficient. A different method of approaching this generative process would be to look further into Probabilistic Graphical Models. Appropriately perturbing the input signal to match the distribution of the generative model may yield meaningful results. </span></p><p class="c2"><span class="c3 c1">References</span><span class="c1">: </span></p><p class="c7"><span class="c8"><a class="c0" href="https://www.google.com/url?q=https://arxiv.org/pdf/1601.06759v3.pdf&amp;sa=D&amp;ust=1481834748450000&amp;usg=AFQjCNGNpsaQKUlH1u17tlDwZ8Bp20SP0A">https://arxiv.org/pdf/1601.06759v3.pdf</a></span></p><p class="c7"><span class="c8"><a class="c0" href="https://www.google.com/url?q=https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html&amp;sa=D&amp;ust=1481834748451000&amp;usg=AFQjCNElrblJOk8LjQBy5tv-_0C31XB_cQ">https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a></span></p><p class="c7 c10"><span></span></p></body></html>